{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Part 4: Markov Chains",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM4JEec8C+0gtR9qL8nOhNN"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Wh-I-__lYV-",
        "colab_type": "text"
      },
      "source": [
        "# Markov Chains\n",
        "\n",
        "The systems we have looked so far were quite simple: there was a single random variable or in the worst case a response variable that depended on it. In the real-world we also come accross a lot of complex systems where the data we observe is due to a series of random variables all interlinked in some way. We will look at one such system in this notebook - Markov Chains. This model forms the basis for many things in computer science - from language and communication channel modelling to recurrent neural networks, so it is important to understand it.\n",
        "\n",
        "## How to recognise a Markov Chain?\n",
        "\n",
        "Markov Chain model applies when there is a (possibly infinite) sequence of data we could observe and one observation is related to the previous one. Original application of this model was poetry where the sequence of data under study were different letters or words. The subsequent letters are obviously related - there is a much higher likelihood to observe a 'h' after a 't' than 'x' (i.e. the pair 'th' is much more common in English than 'tx'). Additionally, Markov Chains may have an underlying variable that we are not seeing. In the case of words this might be the sentence structure (are we currently spelling a verb or a noun). If the author has a goal to write an article (as in sentence part), they are very likely to generate the sequence \"th\"! Let's leave literature explanation for this time - although there is a lot of interesting theory surounding it and illustrate a Markov model with a heart-clenching story.\n",
        "\n",
        "Suppose, you have a rare disease that makes you allergic to daylight. You have no way to see what is the weather outside - whether it is raining or not. Yet each day, a friend comes to visit you and is either carrying an umbrella or is not. Can you infer what is the state of the world using just these observations? Note that it is harder than it sounds - a friend might forget an umbrella during a rainy day or carry it when the sun is shining. \n",
        "\n",
        "In such situation we can apply a special type of Markov Chain model - Hidden Markov Model or HMM. We add the word 'hidden' to denote that there are some states that we are not quite certain about (in this example - the weather) but can guesstimate using observation. Let's work on making it more concrete.\n",
        "\n",
        "Here is a week that you might observe:\n",
        "\n",
        "![](https://docs.google.com/drawings/d/e/2PACX-1vQW2ifjqJegogEUD0fOHNHKlfP58BTfahb9F0BerZ4-_zWYw1oymzzOlHFpAuMgVsfV7eLquGU-GfkR/pub?w=773&h=256)\n",
        "\n",
        "On the top, we have what is trully happening outside and, on the bottom. we have the observations that you are able to make (based on whether your friend carrying an umbrella or not).\n",
        "\n",
        "The arrows illustrate the simplifying assumptions we make. Let's make them explicit:\n",
        "\n",
        "1) A state depends only on the state before. You can see that in the diagram, as each state having \"an arrow of influence\" only to one other state following it and thus we will consider probabilities of the form $Pr(S_i = x | S_{i-1} = y)$. In our story, this means that each new day, the weather either will switch or won't change at all with certain probabilities. \n",
        "\n",
        "2) The observations depend only on the underlying state. That is the downward pointing arrows and we will write this down as $Pr(O_i = z | S_i = x)$. In our case, we assume that the friend each day looks outside and decides whether they will need the ubrella or not (i.e. they do not think about yesterday)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMBYU0kBy_Aa",
        "colab_type": "text"
      },
      "source": [
        "## Specifying Hidden Markov Model\n",
        "\n",
        "From this, we can see that there are four things we need to specify a HMM. \n",
        "\n",
        "1) Identify the set of all possible states $\\mathbb{S}$. Then each state at time-step $i$ will satisfy: $S_i \\in \\mathbb{S}$.\n",
        "\n",
        "2)  Identify the set of all possible observations $\\mathbb{O}$. Then each observation at time-step $i$ will satisfy: $O_i \\in \\mathbb{O}$.\n",
        "\n",
        "3) Calculate the probabilities $Pr(S_i = x | S_{i-1} = y)$ where $x,y \\in \\mathbb{S}$\n",
        "\n",
        "4) Calculate the probabilities $Pr(O_i = z | S_{i} = x)$ where $x \\in \\mathbb{S}$ and $z \\in \\mathbb{O}$\n",
        "\n",
        "We can get these things from our training data. In this case, let's view the week we saw before as our training dataset. Then $\\mathbb{S} = \\{ \\textit{rain}, \\textit{no rain}\\} $ and $\\mathbb{O} = \\{ \\textit{umbrella}, \\textit{no umbrella}\\} $. And the state transition probabilities can be expressed as a table:\n",
        "```\n",
        "Si-1\\Si| rain | no rain\n",
        "rain   | 2/3  | 1/3\n",
        "no rain| 1/3  | 2/3\n",
        "```\n",
        "(Check if you can justify each probability! Be mindful that here we care about the total number of observed transitions from a given state, rather than all transitions or the total number of times the state appears)\n",
        "\n",
        "Similarly observation probabilities (we also call them emission probabilities, maybe, because it rhymes well with transmission probabilities):\n",
        "```\n",
        "    Oi\\Si  | rain | no rain\n",
        "umbrella   | 3/4  | 2/3\n",
        "no umbrella| 1/4  | 1/3\n",
        "```\n",
        "(Do you agree with my calculations?) \n",
        "\n",
        "**Exercise 4.1** Why don't you join in the fun? While I figure out, how to work with weather, here is a challenge for you. I have a fair die and two loaded dice. I have made up a few rules (in terms of probabilities) when I switch between them. You closely observe me playing for a hundred games and write down two sequences:\n",
        "```\n",
        "The die I used: 0200021202111000200211000001002102010122011120100011021000012000111021202101200022212002002100200100\n",
        "What I rolled : 1663351453423546454361636321636116312165211346332242551535136465411543526423513145363356514364611624\n",
        "```\n",
        "I later notice you watching me, so I become careful not to show you how I switch the dice. Here is the sequence of rolls you observe: `52253143166633245541`. Can you tell using HMM when I used loaded dice?\n",
        "\n",
        "We will get to that question later. But your task right now is to figure out the four components of the HMM. Which of the three indexed dice look the fairest?\n",
        "\n",
        "Before you begin, I want to share another useful piece of Python..\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fF3GXL6JDBxo",
        "colab_type": "text"
      },
      "source": [
        "## Dictionaries and Enumerators \n",
        "\n",
        "Dictionaries (also called maps) are quite simple and useful data structures. If you know what arrays and lists are (and you probably know well by now!), then you can think of dictionaries as their generalisations. In lists to number elements we use ... well, numbers, but in dictionaries we can use anything. To be a little less confusing, we call the elements values and their \"number\" - keys. The pair of key and value is called an item. Here is a few examples how to use dictionaries - I also included how dictionary code comparing them to the boring old lists.\n",
        "\n",
        "*Creating a new dictionary*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcQugqN3PSQo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We use square braces fAnnoyingor lists\n",
        "l1 = []\n",
        "\n",
        "# And use curly braces for dictionaries!\n",
        "d1 = {}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Lp5Acp-RnUL",
        "colab_type": "text"
      },
      "source": [
        "*Creating a dictionary with initial values*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzJ46P3iRrj4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# For lists we just *list* initial values between square braces\n",
        "l2 = [1.2, 1.3, 2.2, 3.9]\n",
        "\n",
        "# For dictionaries we list entries, in the form `key : value`\n",
        "d2 = { 1 : 'January', 2 : 'February', 12 : 'December'}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMFKEF02RBgc",
        "colab_type": "text"
      },
      "source": [
        "*Adding an element*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9E7uiLp2RG_b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The annoying thing about lists is that they require to preallocate space before\n",
        "# adding elements. Here is a quick way to create an empty array in Python:\n",
        "l3 = [None]*10 \n",
        "l[1] = 'element'\n",
        "\n",
        "# The good thing with dictionaries, that you can use them straight away!\n",
        "# Just put the key in the square braces and the value after the equals sign.\n",
        "d1['one'] = 'value'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fe2uCxI3So3M",
        "colab_type": "text"
      },
      "source": [
        "*Reading an element*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOQC5wE1SsyV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(l2[2])\n",
        "\n",
        "# For dictionaries just use the key between square braces!\n",
        "print(d2[2])\n",
        "print(d1['one'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjX48XuBTOeb",
        "colab_type": "text"
      },
      "source": [
        "*Iterating*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5YKPExF4TTCb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# For list it is clear what you want to iterate though\n",
        "for n in l2:\n",
        "  print(n)\n",
        "\n",
        "print()\n",
        "\n",
        "# For dictionaries you can get list of keys or values to iterate through\n",
        "list_of_keys = d2.keys()\n",
        "for k in list_of_keys:\n",
        "  print(k)\n",
        "\n",
        "list_of_items = d2.values()\n",
        "for i in list_of_items:\n",
        "  print(i)\n",
        "\n",
        "print()\n",
        "\n",
        "# You can iterate though items as well! \n",
        "\n",
        "for k, v in d2.items():\n",
        "  print(k, '->', v)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mCbjZv-UqSv",
        "colab_type": "text"
      },
      "source": [
        "Note that the diiferent lists derived from a dictionary are useful to check if a key is present!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvAxRS57UzNw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(3 in d2.keys())\n",
        "print(2 in d2.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C50e74ZTVC7p",
        "colab_type": "text"
      },
      "source": [
        "There is nothing stopping you from mixing the types of things you use as keys and values in a Python dictionary - but that is terrible! You should avoid that because that makes your code very confusing and if you do need something like this, it is better to create multiple dictionaries: each one with different \"types\" of data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cp89SFj-VRIQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "terrible = {\n",
        "    'one' : 1,\n",
        "    1.2   : 'one point two',\n",
        "    1.5   : 1\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8n2sHdSLVcuB",
        "colab_type": "text"
      },
      "source": [
        "Enumerators are useful to organise dictionaries.  \n",
        "\n",
        "A lot of the time in code, we want our data to have a lot of meaning and yet be simple to implement and store. For example, instead of using string with month names, you might just use integers which represent month numbers. For months, the correspondance is obvious and you (or your cowokers in the future) are not likely to forget what they mean, but with other data that might not be the case. Consider how should I represent the set $\\mathbb{S}=\\{\\textit{rain}, \\textit{no rain}\\}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pV0wwJBmxXnm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Option 1: spell it out each time:\n",
        "\n",
        "dict_of_important_stuff = {\n",
        "    \"no rain\" : 1.345,\n",
        "    \"rain\"    : 65.43\n",
        "}\n",
        "\n",
        "# Cons: 1) storing a string is more expensive than a number!\n",
        "# 2) In the future it is likely that I will make a spelling mistake:\n",
        "\n",
        "dict_of_important_stuff['no rian'] = 38.1\n",
        "\n",
        "# Option 2: use numbers\n",
        "\n",
        "dict_of_important_stuff = {\n",
        "    1 : 1.345,\n",
        "    0 : 65.43\n",
        "}\n",
        "\n",
        "# This solves number 1 and sort of solves number 2 - each time I want\n",
        "# to use the dictionary I have to remember what I meant by 0 and 1.\n",
        "# This adds delay and fustration on the other hand."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyOsZJ1Zyg8R",
        "colab_type": "text"
      },
      "source": [
        "Thus we turn to Enumerators. New enumerators are defined by subclassing Enum and providing a list of meaningful names with not so meaningful alternative representation like integers. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0FOcn_Gy0Lv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from enum import Enum\n",
        "\n",
        "class WState(Enum):\n",
        "  no_rain = 0\n",
        "  rain = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YeZK56NczlhK",
        "colab_type": "text"
      },
      "source": [
        "To use the enum you just write the class name dot the meaningful name. Autocomplete will remind you what names you have listed and parser will catch any spelling mistakes.\n",
        "\n",
        "Try it out: run code bellow, fix the spelling mistake and print the value associated with rain state in the dictionary!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZM8Z7ZpNzSdo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dict_of_important_stuff = {\n",
        "    WState.no_rain : 1.345,\n",
        "    WState.rain    : 65.43\n",
        "}\n",
        "\n",
        "# Spelling mistakes are caught!\n",
        "dict_of_important_stuff[WState.no_rian]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNT3HT-XYL8E",
        "colab_type": "text"
      },
      "source": [
        "You can use `value` to get the underlying value the enumerated name represents. Iterating through enumerators is also intuitive:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XSAztiLYfO3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "7824e96a-fac6-4d23-fcdd-72fb5cbbba9f"
      },
      "source": [
        "# Btw, enumerator values might be not integers. Making it into a character will \n",
        "# be particular useful for the exercise 4.1\n",
        "class options(Enum):\n",
        "  A = 1.2\n",
        "  B = 3.4\n",
        "\n",
        "for o in options:\n",
        "  print(o.value)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.2\n",
            "3.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QA6_xYNt0N4Z",
        "colab_type": "text"
      },
      "source": [
        "Here is how I would use Enumerators together with Dictionaries to represent the two probability tables:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndDFW_h30avS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class WObservation(Enum):\n",
        "  no_umbrella = 0\n",
        "  umbrella = 1\n",
        "\n",
        "weather_trans = {\n",
        "    WState.rain    : {WState.rain: 2/6, WState.no_rain: 1/6},\n",
        "    WState.no_rain : {WState.rain: 1/6, WState.no_rain: 2/6},\n",
        "}\n",
        "\n",
        "# It is more useful to order emission table by column!\n",
        "# It makes it easy to check that each column sums to 1!\n",
        "\n",
        "weather_emis = {\n",
        "    WState.rain    : {WObservation.umbrella: 3/4, WObservation.no_umbrella: 1/4},\n",
        "    WState.no_rain : {WObservation.umbrella: 2/3, WObservation.no_umbrella: 1/3},\n",
        "}\n",
        "\n",
        "# Do not fear the dictionary in dictionary! Working with them is quite intuitive:\n",
        "print(\"Pr(umbrella|no_rain)=\", weather_emis[WState.no_rain][WObservation.umbrella])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f12U4G6y2aff",
        "colab_type": "text"
      },
      "source": [
        "Now you can go back to **Exercise 4.1**! \n",
        "\n",
        "I know you are dying with impacience to start. Your code will be more complex than mine - I hope that you will use loops to calculate the right probabilities instead of just counting as I did. \n",
        "In the end you should get transition and emission tables in a similar format as I did. Do not be lazy and use all the enumerators, dictionaries and loops, because later you will get a more difficult markov model (protein prediction), so it would be nice if you could reuse some of your code you write here!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfIBynDG32aa",
        "colab_type": "text"
      },
      "source": [
        "## HMM Decoding\n",
        "\n",
        "We have gone through the preparatory steps, not it is time to learn how to decode the most likely state sequence given the observations. \n",
        "\n",
        "$$Pr(S_1=s_1,...,S_{n-1}=s_{n-1}, S_n=s_n|O_1=o_1,...,O_{n-1}=o_{n-1}, O_n=o_n)\n",
        "$$\n",
        "\n",
        "Intuitively, this event are a lot smaller event combined. These smaller event are: \"start at $s_1$\", \"emit $o_1$ from $s_1$\", \"transition from $s_1$ to $s_2$\", \"emit $o_2$ from $s_2$\" and so on. So after simplifying the notation and applying this intuition (you can ask me for formal proof if interested!) we get:\n",
        "\n",
        "$$Pr(s_1,...,s_{n-1},s_n|o_1,...,o_{n-1}, o_n) = Pr(s_1)Pr(o_1|s_1)Pr(s_2|s_1)Pr(o_2|s_2)...Pr(s_n|s_{n-1})Pr(o_n|s_n)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kVVG8yKi-5V",
        "colab_type": "text"
      },
      "source": [
        "### Stationary distribution\n",
        "\n",
        "There is only one misterious thing in the expression above - how to get $Pr(s_1)$. Our approach will be to calculate the probability of ending up in any particular state after the state transitions have been happening *for a while*. We will need to asume that we can eventually reach any state from any other state or the math will get hairy. \n",
        "\n",
        "Thankfully, this \"reachability\" property holds for our Markov states. The key idea to figure out $Pr(s)$ is that the number \"for a while\" and \"for a while minus one\" are the same when we get to infinity. This is when we have truly no better knowledge of the starting state distribution just $Pr(s)$.\n",
        "\n",
        "$$Pr(s)=Pr(S_n = s) = Pr(S_{n-1}=s_1)Pr(s|s_1) + ... + Pr(S_{n-1}=s_m)Pr(s|s_m)$$\n",
        "\n",
        "This expression says that probability of getting to state $s$ at any step $n$ is the same as being in state $s_1$ one step before and making the transition *or* being in state $s_2$ before and making the transition and so on over all the possible states. Using the key idea we get:\n",
        "\n",
        "$$Pr(s)=Pr(s_1)Pr(s|s_1) + ... + Pr(s_m)Pr(s|s_m)$$\n",
        "\n",
        "We also know that probabilities must sum to one:\n",
        "\n",
        "$$Pr(s_1) + ... + Pr(s_m) = 1$$\n",
        "\n",
        "This creates a system of equations which we can solve using library functions! But to use them, you will need to convert your transition dictionary into a 2D array (a matrix). The first index should correspond to the \"previous\" state, the second index - to the \"next\" state. \n",
        "\n",
        "**Exercise 4.2** Write the code to convert your dictionary into the right matrix. When you got the matrix, use it in a similar snippet as the one below to get a list of probabilities!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkxXIDntpq-L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "6446241e-ca2b-4677-a546-5d07e46af3b6"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "nr_of_states = 2\n",
        "\n",
        "M = np.array([\n",
        "     [2/3, 1/3],\n",
        "     [1/3, 2/3]\n",
        "])\n",
        "\n",
        "A = np.concatenate(((M - np.eye(nr_of_states)).transpose(), [np.ones(nr_of_states)]))\n",
        "B = np.concatenate((np.zeros(nr_of_states), [1]))\n",
        "ps = np.linalg.lstsq(A, B, rcond=None)[0]\n",
        "\n",
        "print(\"Pr(rain) = \", ps[WState.rain.value])\n",
        "print(\"Pr(no rain) = \", ps[WState.no_rain.value])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pr(rain) =  0.5\n",
            "Pr(no rain) =  0.4999999999999996\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GT7o9bYOw5GR",
        "colab_type": "text"
      },
      "source": [
        "The probability distribution over states we just got is called the stationary distribution. In the weather case, since the numbers are symetric, the values are not that surprising (you can double check them by solving the system of equations).\n",
        "\n",
        "**Exercise 4.3** Now you have all the necessary ingredient to calculate:\n",
        "\n",
        "$$Pr(S_1=s_1,...,S_{n-1}=s_{n-1}, S_n=s_n|O_1=o_1,...,O_{n-1}=o_{n-1}, O_n=o_n)\n",
        "$$\n",
        "\n",
        "Write a function that would do that! Here is the signature for your function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKixijEgAPNG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Pr_sequence(seq, obs, emis, trans, stat):\n",
        "  \"\"\"\n",
        "  :param seq: (string) - a possible die sequence\n",
        "  :param obs: (string) - observed rolls, same length as seq\n",
        "  :param emis: (dictionary) - emission probability table\n",
        "  :param trans: (dictionary) - transition probability table\n",
        "  :param stat: (list) - stationary state distribution\n",
        "  :return: (real) the probability of seq given obs\n",
        "  \"\"\"\n",
        "  # Your code goes here!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQjZ6oaJi9Yi",
        "colab_type": "text"
      },
      "source": [
        "### Viterbi algorithm\n",
        "\n",
        "Having written the function described in the exercise above, we could plug all possible sequences of hidden states and see which one comes up as the maximum, but we could do something much smarter (and faster) - Viterbi algorithm.\n",
        "\n",
        "For the weather example, let's say that a couple of days have passed after your initial observations. You watch your friend for three days and record that they have come without an umbrella, with an umbrella and without an umbrella in that order. What is the most likely sequence of weather states over those three days?\n",
        "\n",
        "Let's approach this question methodically - that is analyse the new observations one by one. Say, your friend shows up with an umbrella - what is the most likely weather state that day? \n",
        "\n",
        "Here is an illustrative picture:\n",
        "\n",
        "![](https://docs.google.com/drawings/d/e/2PACX-1vTevetQHK0jDodGU-m_asQbZOOZKvELKx5EJ5BFdoWwlrKT9X7KcpTxCmLXb_GB04rF_NwPJVSoWGil/pub?w=443&h=565)\n",
        "\n",
        "We just need to figure out the probability that the first state is `rain` and compare that with `no_rain`. Since we started observing this chain at a random point, we can use the stationary distribution. The probabilities thus are:\n",
        "\n",
        "$$Pr(S_1=rain)=Pr(rain)\\times Pr(umbrella|rain) = 0.5 \\times 0.75 = 0.375$$\n",
        "$$Pr(S_1=\\textit{no rain})=Pr(\\textit{no rain})\\times Pr(umbrella|\\textit{no rain}) = 0.5 \\times \\frac{2}{3} = \\frac{1}{3}$$\n",
        "\n",
        "So that it is raining on the first day is slightly more likely! However we have more states to figure out so we store both of these probabilities. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAHQEOwUiRqQ",
        "colab_type": "text"
      },
      "source": [
        "What is the most likely weather on the second day? \n",
        "\n",
        "Let's first consider the probability $Pr(S_2 = \\textit{no rain})$. There are two ways the state can become `no rain`: either a day before it was rainy and it switched, or it was not rainy and it remained the same. We need to figure which scenario is more likely and store it. Since we care only about the most likely of the sequence we do not need to keep the alternative path.\n",
        "\n",
        "![](https://docs.google.com/drawings/d/e/2PACX-1vS4DcH97IDFX96Q38l3nEUvfewOcMFzx7bCaKDMF1jutWWgkouBpgEXps77RIS4c0hRW1J_uEwwjFiW/pub?w=443&h=565)\n",
        "\n",
        "The probabilities are straight forward:\n",
        "\n",
        "$$Pr(S_2=\\textit{no rain}) = Pr(S_1=\\textit{no_rain})Pr(\\textit{no rain}|\\textit{no rain})Pr(\\textit{no umbrella}|\\textit{no rain})$$\n",
        "\n",
        "Notice the $Pr(S_1 = \\textit{no rain})$ term - we have already worked it out and convenietely stored, so the expression becomes:\n",
        "\n",
        "$$Pr(S_2=\\textit{no rain}) = \\frac{1}{3}\\times\\frac{2}{3}\\times\\frac{1}{3}\\approx 0.074074$$\n",
        "\n",
        "Similarly for going from the `rain` state:\n",
        "\n",
        "$$Pr(S_2=\\textit{no rain}) = Pr(S_1=\\textit{rain})Pr(\\textit{no rain}|\\textit{rain})Pr(\\textit{no umbrella}|\\textit{no rain})=0.375\\times\\frac{1}{3}\\times\\frac{1}{3}\\approx 0.041667$$\n",
        "\n",
        "We see that this is less than transitioning from `no rain` state, so we will save that the most likely previous state is `no rain` and store $0.074074$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRvvJcoBrbec",
        "colab_type": "text"
      },
      "source": [
        "We go through similar calculations for `rain` state.\n",
        "\n",
        "![](https://docs.google.com/drawings/d/e/2PACX-1vTcrct9cmTf9WfLAV0BEIkqy62hUEJg09sz_HYWRa4uBOL4Z19oO69_eESHY1dPowSS_Hv4HFF2gg3N/pub?w=443&h=565)\n",
        "\n",
        "$$Pr(S_2=\\textit{rain}) = Pr(S_1=\\textit{no_rain})Pr(\\textit{rain}|\\textit{no rain})Pr(\\textit{no umbrella}|\\textit{rain})=0.375\\times\\frac{1}{3}\\times0.25\\approx0.03125$$\n",
        "\n",
        "$$Pr(S_2=\\textit{rain}) = Pr(S_1=\\textit{rain})Pr(\\textit{rain}|\\textit{rain})Pr(\\textit{no umbrella}|\\textit{rain})=\\frac{1}{3}\\times\\frac{2}{3}\\times0.25\\approx0.055556$$\n",
        "\n",
        "Once again we save the biggest probability and the transition that it represents.\n",
        "\n",
        "![](https://docs.google.com/drawings/d/e/2PACX-1vQFVrSbxdslXZOZSAUlAiMvghfXG1fM_wM-dBDGfo95s_-TZ4ZkIwXnVH9r4zuWI0o5It0mZUsleDe6/pub?w=443&h=565)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqfxPfuTtBnS",
        "colab_type": "text"
      },
      "source": [
        "We just have one more state left! If you want - try to work out yourself the values we would save for the last pair of possible states. \n",
        "\n",
        "Here is what I get:\n",
        "\n",
        "![](https://docs.google.com/drawings/d/e/2PACX-1vRN-TLzbb4Ij1pGQnUtvZ8JgfDMEe5N7kKupNyAHbwxryB4OIqLytU_O_cCSt0jOJqsXKQKYucj7xOz/pub?w=443&h=565)\n",
        "\n",
        "Once we are done with this process for the entire sequence, we can be sure what sequence is the most probable. We can just look at the probabilities we get for the last possible state, compare which is the larger one, and use the chain of saved states to figure out the whole sequence.\n",
        "\n",
        "In this case, the most probable sequence of states is `rain -> rain -> rain`. Would you have guessed the same? \n",
        "\n",
        "Also, you might notice that the probabilities get lower and lower with each state and that risks loosing precision. So in your experiment with dice use log probabilities and sums instead of multiplications - you know the drill already."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1qXCALPwXl9",
        "colab_type": "text"
      },
      "source": [
        "The process that we just went though is called Viterbi algorithm and it is an example of dynamic programming. Here is a more formal pseudocode (similar to python but not quite working) for it:\n",
        "\n",
        "```\n",
        "Viterbi(obs, trans, emis, stat):\n",
        "  :param obs: (string) - observed rolls\n",
        "  :param emis: (dictionary) - emission probability table\n",
        "  :param trans: (dictionary) - transition probability table\n",
        "  :param stat: (dictionary) - stationary state distribution\n",
        "  n = len(obs)\n",
        "  store = new list of length n\n",
        "  // Deal with first state\n",
        "  store[0] = new dictionary\n",
        "  for s in States:\n",
        "    store[0][s] = (None, log(stat[s]) + log(emis[s][obs[0]]))\n",
        "  // Run trough the rest of observations\n",
        "  for i in [1; n - 1]:\n",
        "    store[i] = new dictionary\n",
        "    for s in States:\n",
        "      max = - infinity \n",
        "      for prev in States:\n",
        "        m = store[i-1][prev][1] + log(trans[prev][s]) \n",
        "            + log(emis[s][Roll(obs[i])])\n",
        "        if m > max:\n",
        "          max = m\n",
        "          state = prev        \n",
        "      store[i][s] = (state, max)\n",
        "  // Backtrack to recover the sequence\n",
        "  seq = new list of length n\n",
        "  seq[n - 1] = the state with the max pr over store[n - 1]\n",
        "  for i in range(2, n):\n",
        "    seq[n - i] = store[n - i][seq[n - i + 1]][0]\n",
        "  // Deal with the first state\n",
        "  seq[0] = store[1][seq[1]][0]\n",
        "  return seq\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18MPox6V1qsn",
        "colab_type": "text"
      },
      "source": [
        "You now should guess what your task is!\n",
        "\n",
        "**Exercise 4.3** \n",
        "Implement Viterbi algorithm to decode the most likely hidden sequence of states for the die observations `52253143166633245541`. Once you have done that you can compare the sequence you got with the real one - `02010021121100000210`. Calculate how many states match - you should get a number around 45% to 60%. That is not a bad result - given that the observation sequence is only 100 observations and there are three possible states."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6loGOLEJqst",
        "colab_type": "text"
      },
      "source": [
        "I hope you now feel comfortable with Markov Chains - or at least familiar enough. Here is a challenge to test out your knowledge! \n",
        "\n",
        "You will have to decode amino acid Markov Chain. The data set is made up of readings (160 of them!) from multiple experiments that recorded which amino acids make up a certain sample. Then these reading were identified as belonging to the cell membrane or being outside or inside of the cell. The goal is to learn to do this matching part automatically just from the reading the sequence of amino acids. Here is an example of how results from one experiment look like:\n",
        "\n",
        "```\n",
        "ATLTAEQSEELHKYVIDGTRVFLGLALVAHFLAFSATPWLH\n",
        "iiiiiiiiiiiiiMMMMMMMMMMMMMMMMMMMMMMMooooo\n",
        "```\n",
        "\n",
        "The amino acids are encoded as letters but you do not have to understand which specific protein the letter corresponds to. This enumerator will help with identifying observations:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWSez24zKwZB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from enum import Enum\n",
        "\n",
        "class AminoAcid(Enum):\n",
        "  ARG = 'R'\n",
        "  HIS = 'H'\n",
        "  LYS = 'K'\n",
        "  ASP = 'D'\n",
        "  GLU = 'E'\n",
        "  SER = 'S'\n",
        "  THR = 'T'\n",
        "  ASN = 'N'\n",
        "  GLN = 'Q'\n",
        "  CYS = 'C'\n",
        "  SEC = 'U'\n",
        "  GLY = 'G'\n",
        "  PRO = 'P'\n",
        "  ALA = 'A'\n",
        "  VAL = 'V'\n",
        "  ILE = 'I'\n",
        "  LEU = 'L'\n",
        "  MET = 'M'\n",
        "  PHE = 'F'\n",
        "  TYR = 'Y'\n",
        "  TRP = 'W'"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEX7Gb99NyUy",
        "colab_type": "text"
      },
      "source": [
        "The hidden state set is quite simple: $\\mathbb{S} = \\{inside, outside, membrane\\}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DoqFBx70Nwvq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Feature(Enum):\n",
        "  Inside = 'i'\n",
        "  Outside = 'o'\n",
        "  Membrane = 'M'"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8eGkHhjOZns",
        "colab_type": "text"
      },
      "source": [
        "Import the file `bio_dataset.txt` to the current runtime using Files tab on the left and execute the following code to get a list of experiment readings. The reading is a pair of two strings - one for the observations, one for the underlying state."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWirrrABO9Ii",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f = open(\"bio_dataset.txt\", \"r\")\n",
        "\n",
        "data = []\n",
        "\n",
        "lines = f.readlines()\n",
        "obs = None\n",
        "seq = None\n",
        "for l in lines:\n",
        "  if len(l) != 1:\n",
        "    if obs == None:\n",
        "       obs = l[1:len(l)-1]\n",
        "    else:\n",
        "       seq = l[0:len(l)-1]\n",
        "  if obs != None and seq != None:\n",
        "    assert(len(obs) == len(seq))\n",
        "    data.append((obs, seq))\n",
        "    obs = None\n",
        "    seq = None\n",
        "\n",
        "f.close()"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxtvbaG3R7ca",
        "colab_type": "text"
      },
      "source": [
        "This time we will not use all of our data! We will choose 10% of the data at random for validation - that is to measure how good your algorithm is. You will train your markov model on the rest 90% and decode the most likely Markov chain for these saved sequences. In the end, you will calculate what percentage of states you got corrrectly for each of the unseen testing sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNWK-t5USEce",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "\n",
        "random.shuffle(data)\n",
        "\n",
        "test = data[0:16]\n",
        "train = data[16:len(data)]"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oHTcagoO0Zo",
        "colab_type": "text"
      },
      "source": [
        "Now you are ready to begin! \n",
        "\n",
        "Step 1: use `train` dataset to calculate transition and emision matrix.\n",
        "Step 2: for each experiment in `test` use amino acid sequence and Viterbi algorithm to decode the hidden states. Then use the true state sequence to calculate how many states you got correctly. Print out this percentage! \n",
        "\n",
        "Once you are done, you can ponder these questions: are there a lot of variantion between the amount of states you got correcly for different sequences? As an algorithm user, would you want to have a lot or little variation? "
      ]
    }
  ]
}